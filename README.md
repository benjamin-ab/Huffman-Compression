# Huffman-Compression
The Huffman Compression with dictionary of words (JAVA)

I have a large collection of raw text files of famous literature, including Leo Tolstoy’s War and Peace consisting of over 3 million characters and about 22000 different words (counting differences in capitalization), and I’d like to store these works more efficiently. David Huffman developed a very efficient method for compressing data based on character frequency in a message. Since all the literature in my collection is in the English language it is reasonable to suppose they will not just have commonly used characters but commonly used words as well. My idea is that if we can treat every distinct word as a symbol and every nonword character (white space, punctuation, etc.) also as a symbol then we can apply Huffman’s encoding based on the frequency of words in the text instead of characters. I propose this will help us compress the text much smaller.
To carry this out we need an efficient way to store each word as we encounter it and the associated count it accumulates as we scan the entire document. While a binary search tree may work, the constant access time of a hash table is more attractive. I will implement a hash table to help store the word counts and eventual codes for each word.
